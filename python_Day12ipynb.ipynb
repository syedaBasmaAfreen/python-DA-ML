{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMuwuKUyAHWbXv7+EfiIqKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedaBasmaAfreen/python-DA-ML/blob/main/python_Day12ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚å® Day12"
      ],
      "metadata": {
        "id": "MFQtbCYT1jAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `NLTK`"
      ],
      "metadata": {
        "id": "lDTes5xM1rXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story=\"This is sentnce 1.This is sentence 2\""
      ],
      "metadata": {
        "id": "hC-rYdYHBAoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing nltk package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "zes-BCs1BV9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "#sent_tokenize is used to split a document or a paragraph into sentence\n",
        "sen_token= sent_tokenize(story)\n",
        "len(sen_token)"
      ],
      "metadata": {
        "id": "2IFgvENoB9Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count =[len(sent.split())for i, sent in enumerate(sen_token)]\n",
        "print(\"number of words in each sentences:\",word_count)\n",
        "Avg_Sent=sum(word_count)//len(word_count)\n",
        "print(\"avrage number of words used in each column:\",Avg_Sent)"
      ],
      "metadata": {
        "id": "CQryu8h2ARtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "wtokens= word_tokenize(story)\n",
        "print(word_tokenize(story))"
      ],
      "metadata": {
        "id": "4UiVwGvw_791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "id": "UQNjk_HDFChp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Removing Punctuations after sentence tokenize`"
      ],
      "metadata": {
        "id": "i-t3U2tREY9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punctuation =str.maketrans('','', string.punctuation)"
      ],
      "metadata": {
        "id": "h5g_c2fcFVFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens=dict()\n",
        "for i,j in enumerate(sentences):\n",
        "    sent_tokens[i] = j.translate(str.maketrans('','',string.punctuation))\n",
        "sent_tokens"
      ],
      "metadata": {
        "id": "xnVHErUHUuS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsqxaFZhZ87K"
      },
      "source": [
        "Average sentence length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summ = 0\n",
        "for i,j in enumerate(sentences):\n",
        "  summ+=len(j.split(' '))\n",
        "print(summ)\n",
        "summ/len(sentences)"
      ],
      "metadata": {
        "id": "uHuTDbMKWPrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_len = []\n",
        "for i,j in enumerate(sentences):\n",
        "  words_len.append(len(j.split(' ')))\n",
        "\n",
        "sum(words_len)/len(sentences)"
      ],
      "metadata": {
        "id": "y7XZ3j3IYXKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaeN09igYdRG"
      },
      "source": [
        "# `Word Level Tokenization`\n",
        "\n",
        "Process of converting total text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X10WI_D4cyL_"
      },
      "source": [
        "Removing Punctuations  after word tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words0=''\n",
        "for i,j in enumerate(sentences):\n",
        "    words0+=j.translate(str.maketrans('','',string.punctuation))\n",
        "    # print(j.translate(str.maketrans('','',string.punctuation)))\n",
        "words0"
      ],
      "metadata": {
        "id": "gtx0aP6iZwJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(words0)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "4CqMSKVfZLwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jFCG5hYG9sz"
      },
      "source": [
        "# ` Stemming`\n",
        "\n",
        "Stemming is the process of converting the words of a text to its non-changing portions.\n",
        "\n",
        "Hint: Refer to the following link for [Stemmer](https://www.nltk.org/book/ch03.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "stemmed = [porter.stem(i) for i in words]\n",
        "type(stemmed)\n",
        "stemmed"
      ],
      "metadata": {
        "id": "1ssat3SyaQD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BUfwJ8yHuNu"
      },
      "source": [
        "# ` Lemmatization`\n",
        "\n",
        "Lemmatization is the process of converting the words of a sentence to its dictionary form\n",
        "\n",
        "Hint: Refer to the following link for [Lemmatizer](https://www.nltk.org/book/ch03.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "SuOitbM6dF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = nltk.WordNetLemmatizer()\n",
        "lemma.lemmatize('studies')"
      ],
      "metadata": {
        "id": "e2Lqih4Qc9rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = nltk.WordNetLemmatizer()\n",
        "lemmatized = [lemma.lemmatize(i) for i in words]\n",
        "lemmatized"
      ],
      "metadata": {
        "id": "_aAq-TCeaQa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjYDKMsI7fR"
      },
      "source": [
        "#### Removing Stopwords\n",
        "\n",
        "Hint: Refer to the following link for [Stopwords](https://stackabuse.com/removing-stop-words-from-strings-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "wru33Wn-aRUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "id": "v9mn5L94gJ9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "0CdBhifMi429"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words=[]\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    removed_stop_words.append(i)\n",
        "removed_stop_words"
      ],
      "metadata": {
        "id": "LvhrLYQ0gny0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(removed_stop_words)"
      ],
      "metadata": {
        "id": "YLUMOidAi6Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMwDBNCeJdKI"
      },
      "source": [
        "#### Plot Frequency Distribution for top 25 words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(words))"
      ],
      "metadata": {
        "id": "QdwyFT1MjTeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = nltk.FreqDist(removed_stop_words)\n",
        "freq.plot(25)"
      ],
      "metadata": {
        "id": "wBeLECU-aSA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq"
      ],
      "metadata": {
        "id": "b-cYpv9olE2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9GGw2XsKWwF"
      },
      "source": [
        "#### Identifying rare words from frequency distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freq.values()\n",
        "freq.items()"
      ],
      "metadata": {
        "id": "SvKIkmLDljkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in freq.items():\n",
        "  print(i,j)"
      ],
      "metadata": {
        "id": "7vkhqI9kl2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare = [i for i,j in freq.items() if j==1]\n",
        "rare[:25]"
      ],
      "metadata": {
        "id": "WvsKHp4BaSYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O37sarOcalJ"
      },
      "source": [
        "# `Parts of Speech:`\n",
        "\n",
        "\n",
        "Given any sentence, you can classify each word as a noun, verb, conjunction, or any other class of words. When there are hundreds of thousands of sentences, even millions, this is obviously a large and tedious task. But it's not one that can't be solved computationally.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "MGgFvQPmaS3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCknvySTH1E"
      },
      "source": [
        "Print the parts of speech for the first 20 words using pos_tag\n",
        "\n",
        "Hint: Refer to the following link for[ parts of speech](https://www.nltk.org/book/ch05.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "id": "Aqqd5zoFmVqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Description of Parts of speech with its short forms"
      ],
      "metadata": {
        "id": "_eJvwQ7Nm2N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('JJ')"
      ],
      "metadata": {
        "id": "Dk_M5R9Nmwxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parts_of_speech = nltk.pos_tag(words[:20])\n",
        "parts_of_speech"
      ],
      "metadata": {
        "id": "xgcZIPufm-kA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}